
## 动机与成效

从第一期的 resnet 分类模型，到第二期的蒙特卡洛方法，如今已是第三期时序差分 Sarsa，一路走来，收获很多。一方面对动作游戏有了更深层次的理解，另外一方面也学到了一些强化学习方面的知识。

在蒙特卡洛项目中，state 的定义由聚类模型预测产生，从业务层面无法解释，所以在本次项目中，我们尝试重新定义 state。

最终训练出来的 policy，与蒙特卡洛项目中的 policy 非常相似，稳定性非常的强，绝大部分时间都在防御，但是发起攻击的时机比蒙特卡洛 policy 要略多一些，所以整个战斗过程也稍快一些。攻击多了之后也产生了负面影响，比如受伤的概率也变大了。果然是做的越多，错的就越多。


## 演示视频

三周目白金的时序差分 Q-Learning 狼 再战 稀世强者苇名弦一郎

https://www.bilibili.com/video//

## 游戏设置

- steam 开始游戏
- 游戏设置：图像设定 -- 屏幕模式设置为窗口，游戏分辨率调整为`1280*720`
- 游戏设置：按键设置 -- 使用道具设置为按键`p`, 动作（长按）吸引设置为按键 `e`. 重置视角/固定目标设置为`q`，跳跃键为`f`(未用到)，垫步/识破键为`空格`，鼠标`左键`攻击，`右键`防御
	如果需要改变按键，可以在 `config/actions_conf.yaml` 文件中依次修改各个动作中的按键.
- 把 `伤药葫芦` 设置为第一个快捷使用的道具, 最好只设置这一个道具
- 游戏设置：网络设定 -- 标题画面启动设定为`离线游玩`，目的是为了关闭残影，满地的残影烦死了。
- 保持游戏窗口在最上层，不要最小化或者被其它的全屏窗口覆盖。

## 思路与问题

虽然算法从蒙特卡洛变成了时序差分，但状态 state 的定义依然是一个比较关键的问题。

直观来讲，可以把 boss 当前正在使用的招式作为当前的状态，只要识别出 boss 的招式，player 即可作出相应的应对，这应该也是人类新手玩家的打法。弦一郎一阶段的招式大概不超过 10 种，看起来又是一个分类或者聚类问题。

可是，所谓的招式是连续多帧的动作组合，一帧动作截个图做分类模型我们经常做，多帧动作截图的集合也可以做分类模型吗？值得尝试一下。

在此项目临近结束，补充 readme 的时候，我们才突然意识到，多个连续动作的截图组合在一起，可以认为是一个视频，业界似乎也有视频分类模型，但这次来不及尝试了，代码都写完了。。。下次吧，也不知道我们的破显卡能不能带得动。

这次做的仍然是单帧动作截图的分类，与之前图片分类项目有所不同。之前项目中的 X 是游戏中的截图，y 是截图产生时，玩家按下的按键(左键攻击、右键防御、空格识破等）；在此项目中，X 仍旧是游戏中的截图，y 是游戏结束之后，人工打的标记，把同一个招式的多个连续动作的截图归于一类，放到同一个文件夹中。

当然，我们也没有把全部招式都标记出来，感觉有点累，只标记了如下的四个关键动作，主要是因为这四个动作只要正确处理，都可以打出伤害，危机危机，危险里面蕴含着机会。

1. 突刺危，boss 先是起跳下击，接后撤突刺。处理方法为防御 + 识破 + 攻击。
2. 擒拿危。处理方法为攻击打断 + 攻击。
3. 飞渡浮舟，也就是 2 连击 + 7 连击，处理方法为防御 + 攻击。
4. 突刺危的变种，boss 原地后撤突刺，出招比较隐蔽。处理方法为识破 + 攻击。

一开始，上面四个关键动作的截图收集的很全面，比如飞渡浮舟，从头到尾把招式中的几十帧都收集下来了，后来发现没啥必要，我们真正需要关注的是招式结尾的几帧。

这四个关键招式外的动作截图都做为分类 0，处理方法为防御。

数据集必然是非常不均衡的，分类 0 截图的数量极大，所以我们对分类 0 做了采样，只录制了一点点数据。

各个分类中的图片数量如下：

`[576, 393, 298, 147, 167]`

这次的分类模型在测试集上的准确率极高，达到了 `0.974`
各个分类的准确率：`eval accuracy:  {'0': 0.968944099378882, '1': 0.970873786407767, '2': 0.9259259259259259, '3': 0.975609756097561, '4': 0.9787234042553191}`

到游戏里面尝试了这个分类模型，发现分类 0 经常会被误识别到其它分类里面去（分类 4 是重灾区），原因是 boss 的其它招式的部分动作确实与上面定义的四个招式中的部分动作有点相似。当然偶尔错误的预测也会带来一些惊喜，比如意外的出刀打断了 boss 的某个招式。

为了解决这个问题，我们悟出了 `信号强度` 的概念：同一招式中连续多个帧的分类应该是相同的，在预测时，连续帧相同分类出现的次数可以称为`信号强度`，信号越强，表示对该招式的预测越准确。
稍微尝试了一下，发现只需要有连续三帧预测出来的分类相同，就可以显著缓解上述招式误识别的问题，但分类 4 仍然效果不佳。

每种状态有独立的动作空间，比如 state-1 突刺危的时候，就没有必要去探索防御动作、普通攻击类的动作，只需要探索识破 + 攻击类的动作即可。

总结一下，我们做的其实是如下的数据转化：

`实时截图 image -> 分类 class(不准确) -> 状态 state(相对准确) -> 动作空间 action_space(各状态独立) -> 动作 action(需要探索) `

看起来，似乎只要针对各个 state 定义一些处理规则，就可以应对 boss 的各种招式了，但有些 action 也值得探索一下，比如识破之后，出刀一次还是两次，抑或是三次四次五次。人类玩家在连续出刀的过程中可以实时观察 boss 状态来决定是否中止连招，但程序却需要事先明确连续出刀几次，因为程序发起的连招动作目前还无法在打击过程中随时中止，必须全部打完才能换下一个动作，难度主要在于 boss 状态的判断。

有没有一种在做 grid search 的感觉。

以上的五个分类 0-4 可以作为状态 0-4，再定义两个新的 state：5 与 6 及其处理方法：
state-5 指的是 player 受到了伤害，处理方法为向后垫步，如果是躺在地上则会快速起身。
state-6 指的是 boss 受到了伤害且 player hp < 60，此时应该去喝血瓶。

state-11-19 是指攻击之后处于防御状态的几个step的分段，主要目的是从state-0中分离出来，得分会更精确。

这几个 states 不需要探索 action，直接使用预定义的处理规则即可。


之前都是 boss 先出招，player 作出应对。现在尝试一下主动进攻，时机应该是boss攻击结束之后，且未准备发动下一次进攻，但是此时机用程序很难判断。
state-10 是指 player posture 从高点下降了一些，有可能 boss 的攻势彻底停止了，如果此时 player 是防御状态，或许可以尝试攻击。

寻找新状态的这个过程，有点像是在做特征工程，需要开脑洞。

从最终结果来看，这种状态定义方式已经比较合理了，但不够全面，未能完整反映出游戏中的真实情况。

最后，训练出来的 policy 在某一个 episode 中表现不一定好，但如果战斗多个 episode，那么整体结果一定是比较好的。 就类似于股票策略，它在某些交易的时候可能是亏损的，但是长期坚持下去一定是会盈利的。

最终的打法为:
立足防御，识别 boss 的 3个招式(1突刺危、2擒拿危擒擒又拿拿、3飞渡浮舟)，利用破绽进行尽可能多的连击，自身架势值不高的时候鼓励进攻

代码越来越复杂了，bug 也越来越多，但由于游戏本身和算法本身的容错率都很高，所以 AI 也可以玩得下去。

## 如何训练

- 确认环境

`python debug_display_game_info.py`

会在 assets 目录中生成 debug_ui_elements.jpg，该图片中会绘制游戏屏幕截图中的各个 window 区域

同时还会弹出一个小的 tk 窗口实时显示 player 与 boss 的 hp，这个功能得感谢原作者。


- 测试：执行某个或者某几个动作

`python test.py`


- 训练 Q-Learning policy

`python train.py`

默认会加载 checkpoint 文件中的训练相关信息以及 Q 和 N，然后在此基础上进行训练。

进入游戏后，按 q 键锁定 boss，按 ] 键开始正式的训练。

如果在命令行中使用了 `--new` 参数，则会从第 0 个 episode 开始重新训练。

每一个 episode 结束之后，把 Q 与 N 保存到 checkpoint 文件中。

时序差分并不需要 N，我们把 N 加上了，主要用于观察Q(s)中每个 action 的出现次数。


- 查看 Q与 N

`python checkpoint.py`



## 预测

进入游戏，

在 cmd 窗口中运行：
```
python main.py 
```

等待模型加载完，tk窗口出现，

按 q 键锁定 boss

按 ] 键, 就会针对敌方的出招自动做出预测动作了。

再次按下 ] 键，会停止预测。

按 Backspace 键，退出程序。


## 人工备份

模型的训练过程与结果主要涉及到如下的几个文件：
- checkpoint.json		记录了当前是哪个 episode，以及完成训练时的时间。
- checkpoint.pkl		存储了 Q 与 N


如果要训练新模型的话，可能需要对老模型的这些数据进行备份。


## 大部分代码和思路来自以下网址，感谢他们

- https://github.com/XR-stb/DQN_WUKONG
- https://github.com/analoganddigital/DQN_play_sekiro
- https://github.com/Sentdex/pygta5
- https://github.com/RongKaiWeskerMA/sekiro_play
- https://github.com/louisnino/RLcode

- https://www.lapis.cafe/posts/ai-and-deep-learning/%E4%BD%BF%E7%94%A8resnet%E8%AE%AD%E7%BB%83%E4%B8%80%E4%B8%AA%E5%9B%BE%E7%89%87%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B
- https://blog.csdn.net/qq_36795658/article/details/100533639
- https://blog.csdn.net/Guo_Python/article/details/134922730

- X 图片 y 按键 分类项目 https://github.com/XuLvXiu/sekiro_classifier_ai
- 蒙特卡洛项目 https://github.com/XuLvXiu/sekiro_rl_mc_ai



