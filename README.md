
## 动机与成效

从第一期的 resnet 分类模型，到第二期的蒙特卡洛方法，到第三期时序差分 Sarsa，如今已是第四期时序差分 Q-Learning， 一路走来，收获很多。一方面对动作游戏有了更深层次的理解，另外一方面也学到了一些强化学习方面的知识。

在本次项目中，我们继续完善 state 的定义，有一种在做特征工程的感觉，头都要秃了。

最终训练出来的 policy，牺牲了一部分稳定性，增加了主动进攻的概率，场面变得稍稍好看一点点，但 player 死亡的概率也随之增加。令人欣慰的是，AI 偶尔能够完成忍杀(打崩 boss 的架势条之后一击必杀)，这说明 AI 取得了巨大的进步，已经离开了新手区。


## 演示视频

三周目白金的时序差分 Q-Learning 狼 再战 稀世强者苇名弦一郎

https://www.bilibili.com/video//

不得不承认，在战斗的后半段 AI 属实是超水平发挥了，平时也打不了这么好。

## 游戏设置

- steam 开始游戏
- 游戏设置：图像设定 -- 屏幕模式设置为窗口，游戏分辨率调整为`1280*720`
- 游戏设置：按键设置 -- 使用道具设置为按键`p`, 动作（长按）吸引设置为按键 `e`. 重置视角/固定目标设置为`q`，跳跃键为`f`(未用到)，垫步/识破键为`空格`，鼠标`左键`攻击，`右键`防御
	如果需要改变按键，可以在 `config/actions_conf.yaml` 文件中依次修改各个动作中的按键.
- 把 `伤药葫芦` 设置为第一个快捷使用的道具, 最好只设置这一个道具
- 游戏设置：网络设定 -- 标题画面启动设定为`离线游玩`，目的是为了关闭残影，满地的残影烦死了。
- 保持游戏窗口在最上层，不要最小化或者被其它的全屏窗口覆盖。

## 思路与问题

### 游戏角色分析
弦一郎其实是个 I 人，他的攻击欲望并不强。如果 player 不主动发起攻击，boss 一般只会低频率的普通攻击，偶尔会使用前摇特别长非常容易应对的擒拿危，在这个过程中，player 几乎不会受伤，只会缓缓积累架势条；一旦 player 的攻击频率变高，boss 怒气值上来之后，它的攻击频率就会增加，而且也会使用一些比较罕见的招式来阴人， 比如下段危和原地突刺危甚至还有变种，新人遇到这种情况难免会手忙脚乱。

### 打法: 
立足防御，识别 boss 的 3个招式(1突刺危、2擒拿危擒擒又拿拿、3飞渡浮舟)，利用破绽进行尽可能多的连击，自身架势值不高的时候鼓励进攻.


### 状态空间 state space
状态 state 的定义，依然是一个比较关键的问题。

在之前 Sarsa 项目中，状态有如下几种：

state-0. 默认状态，防御即可。
state-1. 突刺危，boss 先是起跳下击，接后撤突刺。处理方法为防御 + 识破 + 攻击。
state-2. 擒拿危。处理方法为攻击打断 + 攻击。
state-3. 飞渡浮舟，也就是 2 连击 + 7 连击，处理方法为防御 + 攻击。
state-4. 突刺危的变种，boss 原地后撤突刺，出招比较隐蔽。处理方法为识破 + 攻击。但是分类模型对这个分类的误召回率太高，经常把 state-0 里面的数据召回，所以只能放弃识破+攻击，改为防御。
state-5. player 受伤，有倒地和不倒地的两种情况，统一按照倒地来处理，需要按识破键快速起身，然后后撤。这块处理的也不太好。
state-6. boss 受伤，且 player 血量低于 60，此时后撤喝血瓶，也就是葫芦。

其中，前 5 个状态由分类模型预测产生。

这次再补充几个新的状态：

state-7 指的是 player 攻击对 boss 造成了伤害，此时可以发动追击，再多砍上两刀，砍死弦一郎这个老贼，砍他，浇给。但一般无法造成伤害，收益在于可以增加 boss 的架势条。
state-8 指的是 player 的架势条被打崩，此时硬直了，boss 一般会接下劈危，此时应该向后垫步吧，躲过下劈。这块处理的也不是太好。
state-10 指的是 player 架势条在下降，有可能 boss 的攻势暂时停止了，此时 player 或许可以尝试发起攻击。为了避免 player 发起攻击的时候，boss 也同时发起攻击，我们额外计算了相邻两帧的图像相似度，如果图像不相似，说明 boss 可能正在发起攻击。当然，如果图像过于相似，也可能是 boss 正在原地拉弓射箭。最终的效果比较一般，会错过一些明显的攻击机会，也会出现误攻击的情况。state-10 只会在 boss 怒气值较低的时候出现，但是我们对于 boss 怒气值的模拟一点都不准。

state-11-19 是指攻击之后处于 state-0 的几个 step 的分段，主要目的是把它们从 state-0 中分离出来，得分会更精确。

从最终结果来看，这种状态定义方式已经比较接近人类玩家对游戏的理解了，但不够全面，未能完整反映出游戏中的真实情况。

### 动作空间 action space

看起来，似乎只要针对各个 state 定义一些处理规则，就可以应对各种情况了。但有些 action 也值得探索一下，比如识破之后，攻击几次的收益更高。有没有一种在做 grid search 的感觉。

每种状态都有独立的动作空间，比如 state-1 突刺危的时候，就没有必要去探索防御动作、普通攻击类的动作，只需要探索识破 + 攻击类的动作即可。

基于规则的状态，其动作空间长度为 1， 比如擒拿危，不需要探索，只需使用唯一的动作：二连击。

总结一下，我们做的其实是如下的数据转化：

`实时截图 image -> 分类 class(不准确) -> 状态 state(相对准确) -> 动作空间 action_space(各状态独立) -> 动作 action(需要探索) `


### 问题

目前还存在一些比较明显的问题：

1. player 总是会被卡到角落里面，这是很危险的，一方面分类模型可能预测不准确，另外一方面有时候会丢失锁定，只能人工协助重新锁定 boss。
2. 代码越来越复杂，bug 也越来越多，所幸游戏本身和算法本身的容错率都很高，所以 AI 也可以玩得下去。
3. state 有点过拟合，过度的拟合了弦一郎的招式，几乎没有泛化能力，碰到其它的 boss 是打不下去的。 当然了，人类玩家打完弦一郎再去打其它 boss，同样也是泛化不了的，也需要重新拟合新 boss 的招式。
4. 训练出来的 policy 在某一个 episode 中表现不一定好，出点儿意外就挂了，但如果战斗多个 episode，那么整体结果一定是比较好的。 就类似于股票策略，它在某些交易的时候可能是亏损的，但是长期坚持下去一定是会盈利的。
5. 原地突刺危无法处理，还好它出现的概率不大。
6. 在战斗中仍然会出现 boss 在防御， player 也在防御的情况，两人谁也不出招，一直僵持十几秒钟。战斗场面非常难看，也不知道 boss 在等什么，估计它的 AI 也有些问题，此时显然是应该主动攻击的。
7. player 对于距离没有感知，有时候明明距离 boss 很远，也会出手攻击，自然就会打空。


## 如何训练

- 确认环境

`python debug_display_game_info.py`

会在 assets 目录中生成 debug_ui_elements.jpg，该图片中会绘制游戏屏幕截图中的各个 window 区域

同时还会弹出一个小的 tk 窗口实时显示 player 与 boss 的 hp，这个功能得感谢原作者。


- 测试：执行某个或者某几个动作

`python test.py`

可以去找不死半兵卫练习一下招式


- 训练 Q-Learning policy

`python train.py`

默认会加载 checkpoint 文件中的训练相关信息以及 Q 和 N，然后在此基础上进行训练。

进入游戏后，按 q 键锁定 boss，按 ] 键开始正式的训练，按 Backspace 键，退出程序。

如果在命令行中使用了 `--new` 参数，则会从第 0 个 episode 开始重新训练。

每一个 episode 结束之后，把 Q 与 N 保存到 checkpoint 文件中。

时序差分并不需要 N，我们把 N 加上了，主要用于观察 Q(s) 中每个 action 的出现次数。


- 查看 Q与 N

`python checkpoint.py`



## 预测

进入游戏，

在 cmd 窗口中运行：
```
python main.py 
```

等待模型加载完，tk 窗口出现，

按 q 键锁定 boss

按 ] 键, 就会针对敌方的出招自动做出预测动作了。

再次按下 ] 键，会停止预测。

在游戏过程中很有可能会卡在角落里面或因死亡而丢失视角，需要人工再重新锁定一下。

按 Backspace 键，退出程序。


## 人工备份

模型的训练过程与结果主要涉及到如下的几个文件：
- checkpoint.json		记录了当前是哪个 episode，以及完成训练时的时间。
- checkpoint.pkl		存储了 Q 与 N


如果要训练新模型的话，可能需要对老模型的这些数据进行备份。


## 大部分代码和思路来自以下网址，感谢他们

- https://github.com/XR-stb/DQN_WUKONG
- https://github.com/analoganddigital/DQN_play_sekiro
- https://github.com/Sentdex/pygta5
- https://github.com/RongKaiWeskerMA/sekiro_play
- https://github.com/louisnino/RLcode

- https://www.lapis.cafe/posts/ai-and-deep-learning/%E4%BD%BF%E7%94%A8resnet%E8%AE%AD%E7%BB%83%E4%B8%80%E4%B8%AA%E5%9B%BE%E7%89%87%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B
- https://blog.csdn.net/qq_36795658/article/details/100533639
- https://blog.csdn.net/Guo_Python/article/details/134922730

- X 图片 y 按键 分类项目 https://github.com/XuLvXiu/sekiro_classifier_ai
- 蒙特卡洛项目 https://github.com/XuLvXiu/sekiro_rl_mc_ai
- 时序差分 Sarsa 项目 https://github.com/XuLvXiu/sekiro_rl_td_ai




